name: Old_Daily Financial Data Pipeline

# TRIGGER: Run this automatically or manually
on:
  # 1. Schedule: Runs at 07:00 AM UTC every day (Matches PGGM's morning process)
  schedule:
    - cron: '0 7 * * *'
  # 2. Manual: Allows you to click "Run Workflow" button in GitHub for demos
  workflow_dispatch:

jobs:
  run-data-quality-engine:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Get your code from the repo
      - name: Checkout Code
        uses: actions/checkout@v4

      # Step 2: Install Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Install Dependencies
      - name: Install Libraries
        run: |
          python -m pip install --upgrade pip
          pip install pandas yfinance openpyxl ecbdata PyYAML pytest

      # Step 4: Run Ingestion (Phase 1)
      - name: Run Data Ingestion
        run: |
          # Assumes your script is in a 'src' folder
          python src/finance_script.py
        continue-on-error: false # Fail the job if ingestion breaks

      # Step 5: Run Quality & Reconciliation (Phase 2)
      - name: Run QA and Reconciliation
        run: |
          python src/validate_quality.py

      # Step 6: Save the Reports (The "Artifacts")
      # This saves the CSVs so you can download them from the GitHub UI
      - name: Upload Data Artifacts
        if: always() # Run this even if previous steps fail so you can see logs
        uses: actions/upload-artifact@v4
        with:
          name: daily-data-reports
          path: |
            data/*.csv
            quarantine_report.csv
          retention-days: 5